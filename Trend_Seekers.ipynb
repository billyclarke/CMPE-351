{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trend Seekers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_Cc-T2_HImj"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QG2nQCGGc8F"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "from keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\n",
        "from keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
        "from keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\n",
        "from keras.layers import Reshape, merge, Concatenate, Lambda, Average\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.initializers import Constant\n",
        "from keras.layers.merge import add\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (6,6)\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import json\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF1A9LaeHHDC"
      },
      "source": [
        "# Load data from Google Drive to Google CoLab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDeOwtbCHaW1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\", force_remount=True)\n",
        "df = pd.read_json(\"/content/gdrive/My Drive/351/Final/News_Category_Dataset_v2.json\", lines = True)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cULkNUiHpPG"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nciCaa2oHouQ"
      },
      "source": [
        "print(df.size)\n",
        "\n",
        "cates = df.groupby('category')\n",
        "print(\"total categories:\", cates.ngroups)\n",
        "print(cates.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmX-3ypeIGVA"
      },
      "source": [
        "df.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMAC2dyNKX7Y"
      },
      "source": [
        "df['text'] = df.headline + \" \" + df.short_description\n",
        "textArray = df['text'].to_numpy(dtype=str)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stopWords = stopwords.words('english')\n",
        "cleaned_title = []\n",
        "\n",
        "for i in range(textArray.shape[0]):\n",
        "    title_sentence = re.sub('[^A-Za-z0-9 ]+', '', textArray[i])\n",
        "    title_tokens = nltk.word_tokenize(title_sentence)\n",
        "    title_tokens = [w for w in title_tokens if not w.lower() in stopWords]\n",
        "    title_lemmatized_word = [lemmatizer.lemmatize(word.lower()) for word in title_tokens]\n",
        "    cleaned_title.append(\" \".join(word.lower() for word in title_lemmatized_word))\n",
        "cleaned_title = np.asarray(cleaned_title, dtype=object)\n",
        "\n",
        "\n",
        "tk = Tokenizer()\n",
        "tk.fit_on_texts(cleaned_title)\n",
        "X = tk.texts_to_sequences(cleaned_title)\n",
        "df['words'] = X\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHqUprjpK8nU"
      },
      "source": [
        "categories = df.groupby('category').size().index.tolist()\n",
        "categoryInt = {}\n",
        "intCategory = {}\n",
        "for i, k in enumerate(categories):\n",
        "    categoryInt.update({k:i})\n",
        "    intCategory.update({i:k})\n",
        "\n",
        "df['catUpdate'] = df['category'].apply(lambda x: categoryInt[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6KDkygROrBV"
      },
      "source": [
        "X = np.array(X)\n",
        "Y = np_utils.to_categorical(list(df.catUpdate))\n",
        "\n",
        "seed = 29\n",
        "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0fOZ68ROwLL"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9wybxefOxzj"
      },
      "source": [
        "\n",
        "def normalize_text(n):\n",
        "    n = n.lower()\n",
        "    n = re.sub('\\s\\W',' ',n)\n",
        "    n = re.sub('\\W\\s',' ',n)\n",
        "    n = re.sub('\\s+',' ',n)\n",
        "    \n",
        "    return n\n",
        "\n",
        "df['normHead'] = [normalize_text(s) for n in df['headline']]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "x = vectorizer.fit_transform(df['normHead'])\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['category'])\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(x_train, y_train)\n",
        "\n",
        "print(nb.score(x_test, y_test)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq-FNkZ1Pu55"
      },
      "source": [
        "# Text CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oxQ-QtGPwmA"
      },
      "source": [
        "wordIndex = tk.word_index\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "embeddingsIndex = {}\n",
        "f = open('/content/gdrive/My Drive/351/Final/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:], dtype='float32')\n",
        "    embeddingsIndex[word] = vector\n",
        "f.close()\n",
        "\n",
        "print('Unique tokens: %s' % len(wordIndex))\n",
        "print('Word vectors %s' % len(embeddingsIndex))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty7UxleWP0vt"
      },
      "source": [
        "embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
        "for word, i in wordIndex.items():\n",
        "    embeddingVector = embeddingsIndex.get(word)\n",
        "    if embeddingVector is not None:\n",
        "        embeddingMatrix[i] = embeddingVector\n",
        "\n",
        "embedding_layer = Embedding(len(wordIndex)+1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embeddingMatrix),\n",
        "                            input_length=maxlen,\n",
        "                            trainable=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4k10aaZP2ij"
      },
      "source": [
        "inp = Input(shape=(maxlen,), dtype='int32')\n",
        "embedding = embedding_layer(inp)\n",
        "stacks = []\n",
        "for kernel_size in [2, 3, 4]:\n",
        "    conv = Conv1D(64, kernel_size, padding='same', activation='relu', strides=1)(embedding)\n",
        "    pool = MaxPooling1D(pool_size=3)(conv)\n",
        "    drop = Dropout(0.5)(pool)\n",
        "    stacks.append(drop)\n",
        "\n",
        "merged = Concatenate()(stacks)\n",
        "flatten = Flatten()(merged)\n",
        "drop = Dropout(0.5)(flatten)\n",
        "outp = Dense(len(int_category), activation='softmax')(drop)\n",
        "\n",
        "TextCNN = Model(inputs=inp, outputs=outp)\n",
        "TextCNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "TextCNN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm-ychnKP5c2"
      },
      "source": [
        "textcnn_history = TextCNN.fit(x_train, \n",
        "                              y_train, \n",
        "                              batch_size=128, \n",
        "                              epochs=5, \n",
        "                              validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrBn-EabP7aR"
      },
      "source": [
        "acc = textcnn_history.history['accuracy']\n",
        "val_acc = textcnn_history.history['val_accuracy']\n",
        "loss = textcnn_history.history['loss']\n",
        "val_loss = textcnn_history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.plot(epochs, acc, 'red', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Training and validation loss')\n",
        "plt.plot(epochs, loss, 'red', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQgqaiEsUJ1Z"
      },
      "source": [
        "# Attention LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Aw7slFhUKr3"
      },
      "source": [
        "import keras as keras\n",
        "class Attention(keras.layers.Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        self.W = self.add_weight(shape = (input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='kernel',\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape = (input_shape[1],),\n",
        "                                     initializer='zeros',\n",
        "                                     name='bias',\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "        eij = K.tanh(eij)\n",
        "        a = K.exp(eij)\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim\n",
        "    \n",
        "\n",
        "lstm_layer = LSTM(300, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)\n",
        "\n",
        "inp = Input(shape=(maxlen,), dtype='int32')\n",
        "embedding= embedding_layer(inp)\n",
        "x = lstm_layer(embedding)\n",
        "x = Dropout(0.25)(x)\n",
        "merged = Attention(maxlen)(x)\n",
        "merged = Dense(64, activation='relu')(merged)\n",
        "merged = Dropout(0.25)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "outp = Dense(len(int_category), activation='softmax')(merged)\n",
        "\n",
        "AttentionLSTM = Model(inputs=inp, outputs=outp)\n",
        "AttentionLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "AttentionLSTM.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJgeClXAUTE_"
      },
      "source": [
        "attlstm_history = AttentionLSTM.fit(x_train, \n",
        "                                    y_train, \n",
        "                                    batch_size=128, \n",
        "                                    epochs=10, \n",
        "                                    validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21voQUA2UU99"
      },
      "source": [
        "acc = attlstm_history.history['acc']\n",
        "val_acc = attlstm_history.history['val_acc']\n",
        "loss = attlstm_history.history['loss']\n",
        "val_loss = attlstm_history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.title('Training Over Time')\n",
        "plt.plot(epochs, acc, 'red', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'green', label='Testing Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Training and validation loss')\n",
        "plt.plot(epochs, loss, 'red', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}